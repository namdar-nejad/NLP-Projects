{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03116c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/namdar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/namdar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/namdar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "posFilePath = \"./data/rt-polarity.pos\"\n",
    "negFilePath = \"./data/rt-polarity.neg\"\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0f3b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep\n",
    "\n",
    "# import data\n",
    "pos_input = np.loadtxt(posFilePath, dtype=str, delimiter='\\n', encoding=\"latin-1\")\n",
    "neg_input = np.loadtxt(negFilePath, dtype=str, delimiter='\\n', encoding=\"latin-1\")\n",
    "\n",
    "# create dataframes\n",
    "pos_df = pd.DataFrame({'Review' : pos_input, 'Sentiment' : 'positive'})\n",
    "neg_df = pd.DataFrame({'Review' : neg_input, 'Sentiment' : 'negative'})\n",
    "all_df = pos_df.append(neg_df).reset_index()\n",
    "del all_df['index']\n",
    "all_df = all_df.sample(frac = 1)\n",
    "\n",
    "# encoding Sentiment column\n",
    "le = LabelEncoder()\n",
    "all_df[\"Sentiment\"] = le.fit_transform(all_df[\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd59541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing Functions\n",
    "\n",
    "# POS tagging\n",
    "def get_pos(word):\n",
    "    if word.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif word.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif word.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif word.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e3c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization with POS\n",
    "def lemmatize(words):\n",
    "    \n",
    "    pos_list = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    \n",
    "    rtn = []\n",
    "    for word in pos_list:\n",
    "        if get_pos(word[1]) is None:\n",
    "            rtn.append(word[0])\n",
    "        else:\n",
    "            rtn.append(WordNetLemmatizer().lemmatize(word[0], get_pos(word[1])))\n",
    "            \n",
    "    rtn = \" \".join(rtn)\n",
    "        \n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "067b2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "def stem(words):\n",
    "    \n",
    "    rtn = []\n",
    "    split = words.split()\n",
    "    \n",
    "    for word in split:\n",
    "        rtn.append(PorterStemmer().stem(word))\n",
    "        \n",
    "    rtn = \" \".join(rtn)\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a10d6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "def rm_stop(words):\n",
    "    \n",
    "    split = words.split()\n",
    "    rtn = []\n",
    "    \n",
    "    for word in split:\n",
    "        if word not in stopword_list:\n",
    "            rtn.append(word)\n",
    "            \n",
    "    rtn = \" \".join(rtn)\n",
    "        \n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae1a6d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main preprocessing function\n",
    "def preprocess_corpus(corpus, clean=False, stop=False, lemmatization=False,\n",
    "                     stemming=False, special=False):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    \n",
    "    for line in corpus:\n",
    "            \n",
    "        # clean text\n",
    "        if clean:\n",
    "            \n",
    "            # remove HTML tags\n",
    "            line = re.sub('<[^>]*>','',line)\n",
    "            \n",
    "            # lowercase the text    \n",
    "            line = line.lower()\n",
    "            \n",
    "            # remove extra newlines\n",
    "            line = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', line)\n",
    "            \n",
    "        #remove special chars\n",
    "        if special:         \n",
    "            line = re.sub('[^a-zA-z0-9\\s]', '', line)\n",
    "            \n",
    "        # remove stop words\n",
    "        if stop:\n",
    "            line = rm_stop(line)\n",
    "        \n",
    "        # lemmatize text\n",
    "        if lemmatization:\n",
    "            line = lemmatize(line)\n",
    "        \n",
    "        # stem text\n",
    "        if stemming:\n",
    "            line = stem(line)\n",
    "            \n",
    "        # clean text\n",
    "        if clean:\n",
    "            # remove extra whitespace\n",
    "            line = re.sub(' +', ' ', line)\n",
    "        \n",
    "        normalized_corpus.append(line)\n",
    "        \n",
    "    return normalized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40a7ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to slip data and create testing and traning set\n",
    "def split_data(train_percent):\n",
    "\n",
    "    testSpltIndx=int(((len(pos_df) + len(neg_df))*(train_percent/100)))\n",
    "    \n",
    "    reviews = np.array(all_df['Review'])\n",
    "    sentiments = np.array(all_df['Sentiment'])\n",
    "\n",
    "    train_reviews = reviews[:testSpltIndx]\n",
    "    train_sentiments = sentiments[:testSpltIndx]\n",
    "\n",
    "    test_reviews = reviews[testSpltIndx:]\n",
    "    test_sentiments = sentiments[testSpltIndx:]\n",
    "    \n",
    "    return train_reviews, train_sentiments, test_reviews, test_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b485a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Engineering\n",
    "def build_feature(processed_train_reviews, processed_test_reviews):\n",
    "\n",
    "    # BOW approach\n",
    "    vec = CountVectorizer()\n",
    "    # build train features\n",
    "    train_features = vec.fit_transform(processed_train_reviews)\n",
    "\n",
    "    # build test features\n",
    "    test_features = vec.transform(processed_test_reviews)\n",
    "    \n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c99ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "def build_model(train_features, train_sentiments, test_features):\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(max_iter=200)\n",
    "    # Logistic Regression model on BOW features\n",
    "    lr.fit(train_features,train_sentiments)\n",
    "\n",
    "    # predict using model\n",
    "    predictions = lr.predict(test_features)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77a6eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "def train(processed_train_reviews, processed_test_reviews):\n",
    "    \n",
    "    # build features\n",
    "    train_features, test_features = build_feature(processed_train_reviews, processed_test_reviews)\n",
    "    \n",
    "    # build_model\n",
    "    predictions = build_model(train_features, train_sentiments, test_features)\n",
    "    \n",
    "    return test_sentiments, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33782ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to print results \n",
    "def print_res(predictions, test_sentiments):\n",
    "    print('Accuracy:  {:2.5%} '.format(metrics.accuracy_score(test_sentiments, predictions)))\n",
    "    print('Precision: {:2.5%} '.format(metrics.precision_score(test_sentiments, predictions, average='weighted')))\n",
    "    print('Recall:    {:2.5%} '.format(metrics.recall_score(test_sentiments, predictions, average='weighted')))\n",
    "    print('F1 Score:  {:2.5%} '.format(metrics.f1_score(test_sentiments, predictions, average='weighted')))\n",
    "    \n",
    "    return (metrics.accuracy_score(test_sentiments, predictions)), (metrics.precision_score(test_sentiments, predictions, average='weighted')), (metrics.recall_score(test_sentiments, predictions, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9ecc9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "90% Train 10% Test\n",
      "clean=False, stop=False, lemmatization=False, stemming=False, special=False\n",
      "Accuracy:  75.44517% \n",
      "Precision: 75.44925% \n",
      "Recall:    75.44517% \n",
      "F1 Score:  75.44694% \n",
      " \n",
      "90% Train 10% Test\n",
      "clean=True, stop=False, lemmatization=False, stemming=False, special=False\n",
      "Accuracy:  75.44517% \n",
      "Precision: 75.44925% \n",
      "Recall:    75.44517% \n",
      "F1 Score:  75.44694% \n",
      " \n",
      "90% Train 10% Test\n",
      "clean=False, stop=True, lemmatization=False, stemming=False, special=False\n",
      "Accuracy:  75.25773% \n",
      "Precision: 75.25434% \n",
      "Recall:    75.25773% \n",
      "F1 Score:  75.25577% \n",
      " \n",
      "90% Train 10% Test\n",
      "clean=False, stop=False, lemmatization=True, stemming=False, special=False\n",
      "Accuracy:  75.72634% \n",
      "Precision: 75.73266% \n",
      "Recall:    75.72634% \n",
      "F1 Score:  75.72890% \n",
      " \n",
      "90% Train 10% Test\n",
      "clean=False, stop=False, lemmatization=False, stemming=True, special=False\n",
      "Accuracy:  75.82006% \n",
      "Precision: 75.85505% \n",
      "Recall:    75.82006% \n",
      "F1 Score:  75.82796% \n",
      " \n",
      "90% Train 10% Test\n",
      "clean=False, stop=False, lemmatization=False, stemming=False, special=True\n",
      "Accuracy:  74.88285% \n",
      "Precision: 74.88285% \n",
      "Recall:    74.88285% \n",
      "F1 Score:  74.88285% \n",
      " \n",
      "90% Train 10% Test\n",
      "clean=True, stop=False, lemmatization=True, stemming=False, special=True\n",
      "Accuracy:  75.53889% \n",
      "Precision: 75.57809% \n",
      "Recall:    75.53889% \n",
      "F1 Score:  75.54728% \n",
      " \n",
      "90% Train 10% Test\n",
      "clean=True, stop=False, lemmatization=False, stemming=True, special=True\n",
      "Accuracy:  76.38238% \n",
      "Precision: 76.39643% \n",
      "Recall:    76.38238% \n",
      "F1 Score:  76.38699% \n",
      " \n",
      "90% Train 10% Test\n",
      "clean=False, stop=True, lemmatization=False, stemming=True, special=True\n",
      "Accuracy:  75.82006% \n",
      "Precision: 75.88233% \n",
      "Recall:    75.82006% \n",
      "F1 Score:  75.82962% \n"
     ]
    }
   ],
   "source": [
    "## HERE IS WHERE WE ARE RUNNING SOME EXAMPLES ##\n",
    "\n",
    "# this arry will contain the percentage of the traning set we are going to have\n",
    "# to test on other traning splits add the traning set percentage to the array\n",
    "traning_set_splits = [90]\n",
    "\n",
    "# this array contains the preprocessing tags we are going to use\n",
    "# in order the tasgs refer to [clean, stop, lemmatization, stemming, special]\n",
    "preprcessing_configs = [\n",
    "\t    [[False, False, False, False, False]],\n",
    "\t        \n",
    "\t    [[True, False, False, False, False],\n",
    "\t    [False, True, False, False, False],\n",
    "\t    [False, False, True, False, False], \n",
    "\t    [False, False, False, True, False],\n",
    "\t    [False, False, False, False, True]],\n",
    "\t              \n",
    "\t    [[True, False, True, False, True],\n",
    "\t    [True, False, False, True, True],\n",
    "\t    [False, True, False, True, True]]\n",
    "            ]\n",
    "\n",
    "# Run and evaluate\n",
    "results = []\n",
    "\n",
    "for i in traning_set_splits:\n",
    "    for m in preprcessing_configs:\n",
    "        \n",
    "        config = []\n",
    "        percent = []\n",
    "        accuracy = []\n",
    "\n",
    "        for j in m:\n",
    "\n",
    "            # slip data and create testing and traning set\n",
    "            train_reviews, train_sentiments, test_reviews, test_sentiments = split_data(i)\n",
    "\n",
    "            # preprocess datasets\n",
    "            processed_train_reviews = preprocess_corpus(train_reviews, j[0], j[1], j[2], j[3], j[4])\n",
    "            processed_test_reviews = preprocess_corpus(test_reviews, j[0], j[1], j[2], j[3], j[4])\n",
    "            \n",
    "            test_sentiments, predictions = train(processed_train_reviews, processed_test_reviews)\n",
    "            \n",
    "            # print results and data split info\n",
    "            print(\" \")\n",
    "            print(str(i) + \"% Train \" + str(100-i) +\"% Test\" )\n",
    "            print(\"clean=%s, stop=%s, lemmatization=%s, stemming=%s, special=%s\" % (j[0], j[1], j[2], j[3], j[4]))\n",
    "\n",
    "            acc, prec, rec = print_res(predictions, test_sentiments)\n",
    "\n",
    "            # store results for later on\n",
    "            config.append(\"clean=%s, stop=%s, lemmatization=%s, stemming=%s, special=%s\" % (j[0], j[1], j[2], j[3], j[4]))\n",
    "            accuracy.append(round(acc*100,2))\n",
    "            percent.append((i,100-i))\n",
    "        \n",
    "            output = {'Config':config, 'Percent':percent, 'Accuracy':accuracy}\n",
    "            output_df = pd.DataFrame(output)\n",
    "            \n",
    "        results.append(output_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ac83427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Config</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clean=False, stop=False, lemmatization=False, stemming=False, special=False</td>\n",
       "      <td>(90, 10)</td>\n",
       "      <td>75.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        Config  \\\n",
       "0  clean=False, stop=False, lemmatization=False, stemming=False, special=False   \n",
       "\n",
       "    Percent  Accuracy  \n",
       "0  (90, 10)     75.45  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Config</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clean=False, stop=False, lemmatization=False, stemming=False, special=True</td>\n",
       "      <td>(90, 10)</td>\n",
       "      <td>74.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clean=False, stop=True, lemmatization=False, stemming=False, special=False</td>\n",
       "      <td>(90, 10)</td>\n",
       "      <td>75.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clean=True, stop=False, lemmatization=False, stemming=False, special=False</td>\n",
       "      <td>(90, 10)</td>\n",
       "      <td>75.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clean=False, stop=False, lemmatization=True, stemming=False, special=False</td>\n",
       "      <td>(90, 10)</td>\n",
       "      <td>75.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clean=False, stop=False, lemmatization=False, stemming=True, special=False</td>\n",
       "      <td>(90, 10)</td>\n",
       "      <td>75.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Config  \\\n",
       "4  clean=False, stop=False, lemmatization=False, stemming=False, special=True   \n",
       "1  clean=False, stop=True, lemmatization=False, stemming=False, special=False   \n",
       "0  clean=True, stop=False, lemmatization=False, stemming=False, special=False   \n",
       "2  clean=False, stop=False, lemmatization=True, stemming=False, special=False   \n",
       "3  clean=False, stop=False, lemmatization=False, stemming=True, special=False   \n",
       "\n",
       "    Percent  Accuracy  \n",
       "4  (90, 10)     74.88  \n",
       "1  (90, 10)     75.26  \n",
       "0  (90, 10)     75.45  \n",
       "2  (90, 10)     75.73  \n",
       "3  (90, 10)     75.82  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Config</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clean=True, stop=False, lemmatization=True, stemming=False, special=True</td>\n",
       "      <td>(90, 10)</td>\n",
       "      <td>75.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clean=False, stop=True, lemmatization=False, stemming=True, special=True</td>\n",
       "      <td>(90, 10)</td>\n",
       "      <td>75.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clean=True, stop=False, lemmatization=False, stemming=True, special=True</td>\n",
       "      <td>(90, 10)</td>\n",
       "      <td>76.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     Config  \\\n",
       "0  clean=True, stop=False, lemmatization=True, stemming=False, special=True   \n",
       "2  clean=False, stop=True, lemmatization=False, stemming=True, special=True   \n",
       "1  clean=True, stop=False, lemmatization=False, stemming=True, special=True   \n",
       "\n",
       "    Percent  Accuracy  \n",
       "0  (90, 10)     75.54  \n",
       "2  (90, 10)     75.82  \n",
       "1  (90, 10)     76.38  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print all the results orderd by 'Accuracy'\n",
    "for i in results:\n",
    "    pd.set_option('max_colwidth', 500)\n",
    "\n",
    "    output_df.style.set_properties(subset=['Config'], **{'width': '600px'})\n",
    "\n",
    "    display(i.sort_values('Accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c42cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/namdar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/namdar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/namdar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "90% Train 10% Test\n",
      "clean=False, stop=False, lemmatization=False, stemming=False, special=False\n",
      "Accuracy:  77.97563% \n",
      "Precision: 77.97292% \n",
      "Recall:    77.97563% \n",
      "F1 Score:  77.97366% \n",
      " \n",
      "90% Train 10% Test\n",
      "clean=True, stop=False, lemmatization=False, stemming=False, special=False\n",
      "Accuracy:  77.97563% \n",
      "Precision: 77.97292% \n",
      "Recall:    77.97563% \n",
      "F1 Score:  77.97366% \n",
      " \n",
      "90% Train 10% Test\n",
      "clean=False, stop=True, lemmatization=False, stemming=False, special=False\n",
      "Accuracy:  77.03843% \n",
      "Precision: 77.04303% \n",
      "Recall:    77.03843% \n",
      "F1 Score:  77.04012% \n",
      " \n",
      "90% Train 10% Test\n",
      "clean=False, stop=False, lemmatization=True, stemming=False, special=False\n",
      "Accuracy:  78.06935% \n",
      "Precision: 78.06573% \n",
      "Recall:    78.06935% \n",
      "F1 Score:  78.06507% \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "posFilePath = \"./data/rt-polarity.pos\"\n",
    "negFilePath = \"./data/rt-polarity.neg\"\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "\n",
    "# Data Prep\n",
    "\n",
    "# import data\n",
    "pos_input = np.loadtxt(posFilePath, dtype=str, delimiter='\\n', encoding=\"latin-1\")\n",
    "neg_input = np.loadtxt(negFilePath, dtype=str, delimiter='\\n', encoding=\"latin-1\")\n",
    "\n",
    "# create dataframes\n",
    "pos_df = pd.DataFrame({'Review' : pos_input, 'Sentiment' : 'positive'})\n",
    "neg_df = pd.DataFrame({'Review' : neg_input, 'Sentiment' : 'negative'})\n",
    "all_df = pos_df.append(neg_df).reset_index()\n",
    "del all_df['index']\n",
    "all_df = all_df.sample(frac = 1)\n",
    "\n",
    "# encoding Sentiment column\n",
    "le = LabelEncoder()\n",
    "all_df[\"Sentiment\"] = le.fit_transform(all_df[\"Sentiment\"])\n",
    "\n",
    "\n",
    "# Pre-processing Functions\n",
    "\n",
    "# POS tagging\n",
    "def get_pos(word):\n",
    "    if word.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif word.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif word.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif word.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# lemmatization with POS\n",
    "def lemmatize(words):\n",
    "    \n",
    "    pos_list = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    \n",
    "    rtn = []\n",
    "    for word in pos_list:\n",
    "        if get_pos(word[1]) is None:\n",
    "            rtn.append(word[0])\n",
    "        else:\n",
    "            rtn.append(WordNetLemmatizer().lemmatize(word[0], get_pos(word[1])))\n",
    "            \n",
    "    rtn = \" \".join(rtn)\n",
    "        \n",
    "    return rtn\n",
    "\n",
    "# stemming\n",
    "def stem(words):\n",
    "    \n",
    "    rtn = []\n",
    "    split = words.split()\n",
    "    \n",
    "    for word in split:\n",
    "        rtn.append(PorterStemmer().stem(word))\n",
    "        \n",
    "    rtn = \" \".join(rtn)\n",
    "    return rtn\n",
    "\n",
    "# remove stop words\n",
    "def rm_stop(words):\n",
    "    \n",
    "    split = words.split()\n",
    "    rtn = []\n",
    "    \n",
    "    for word in split:\n",
    "        if word not in stopword_list:\n",
    "            rtn.append(word)\n",
    "            \n",
    "    rtn = \" \".join(rtn)\n",
    "        \n",
    "    return rtn\n",
    "\n",
    "# main preprocessing function\n",
    "def preprocess_corpus(corpus, clean=False, stop=False, lemmatization=False,\n",
    "                     stemming=False, special=False):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    \n",
    "    for line in corpus:\n",
    "            \n",
    "        # clean text\n",
    "        if clean:\n",
    "            \n",
    "            # remove HTML tags\n",
    "            line = re.sub('<[^>]*>','',line)\n",
    "            \n",
    "            # lowercase the text    \n",
    "            line = line.lower()\n",
    "            \n",
    "            # remove extra newlines\n",
    "            line = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', line)\n",
    "            \n",
    "        #remove special chars\n",
    "        if special:         \n",
    "            line = re.sub('[^a-zA-z0-9\\s]', '', line)\n",
    "            \n",
    "        # remove stop words\n",
    "        if stop:\n",
    "            line = rm_stop(line)\n",
    "        \n",
    "        # lemmatize text\n",
    "        if lemmatization:\n",
    "            line = lemmatize(line)\n",
    "        \n",
    "        # stem text\n",
    "        if stemming:\n",
    "            line = stem(line)\n",
    "            \n",
    "        # clean text\n",
    "        if clean:\n",
    "            # remove extra whitespace\n",
    "            line = re.sub(' +', ' ', line)\n",
    "        \n",
    "        normalized_corpus.append(line)\n",
    "        \n",
    "    return normalized_corpus\n",
    "\n",
    "# function to slip data and create testing and traning set\n",
    "def split_data(train_percent):\n",
    "\n",
    "    testSpltIndx=int(((len(pos_df) + len(neg_df))*(train_percent/100)))\n",
    "    \n",
    "    reviews = np.array(all_df['Review'])\n",
    "    sentiments = np.array(all_df['Sentiment'])\n",
    "\n",
    "    train_reviews = reviews[:testSpltIndx]\n",
    "    train_sentiments = sentiments[:testSpltIndx]\n",
    "\n",
    "    test_reviews = reviews[testSpltIndx:]\n",
    "    test_sentiments = sentiments[testSpltIndx:]\n",
    "    \n",
    "    return train_reviews, train_sentiments, test_reviews, test_sentiments\n",
    "\n",
    "\n",
    "# Features Engineering\n",
    "def build_feature(processed_train_reviews, processed_test_reviews):\n",
    "\n",
    "    # BOW approach\n",
    "    vec = CountVectorizer()\n",
    "    # build train features\n",
    "    train_features = vec.fit_transform(processed_train_reviews)\n",
    "\n",
    "    # build test features\n",
    "    test_features = vec.transform(processed_test_reviews)\n",
    "    \n",
    "    return train_features, test_features\n",
    "\n",
    "\n",
    "# Build Model\n",
    "def build_model(train_features, train_sentiments, test_features):\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(max_iter=200)\n",
    "    # Logistic Regression model on BOW features\n",
    "    lr.fit(train_features,train_sentiments)\n",
    "\n",
    "    # predict using model\n",
    "    predictions = lr.predict(test_features)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Train Model\n",
    "def train(processed_train_reviews, processed_test_reviews):\n",
    "    \n",
    "    # build features\n",
    "    train_features, test_features = build_feature(processed_train_reviews, processed_test_reviews)\n",
    "    \n",
    "    # build_model\n",
    "    predictions = build_model(train_features, train_sentiments, test_features)\n",
    "    \n",
    "    return test_sentiments, predictions\n",
    "\n",
    "\n",
    "# Funtion to print results \n",
    "def print_res(predictions, test_sentiments):\n",
    "    print('Accuracy:  {:2.5%} '.format(metrics.accuracy_score(test_sentiments, predictions)))\n",
    "    print('Precision: {:2.5%} '.format(metrics.precision_score(test_sentiments, predictions, average='weighted')))\n",
    "    print('Recall:    {:2.5%} '.format(metrics.recall_score(test_sentiments, predictions, average='weighted')))\n",
    "    print('F1 Score:  {:2.5%} '.format(metrics.f1_score(test_sentiments, predictions, average='weighted')))\n",
    "    \n",
    "    return (metrics.accuracy_score(test_sentiments, predictions)), (metrics.precision_score(test_sentiments, predictions, average='weighted')), (metrics.recall_score(test_sentiments, predictions, average='weighted'))\n",
    "\n",
    "\n",
    "## HERE IS WHERE WE ARE RUNNING SOME EXAMPLES ##\n",
    "\n",
    "# this arry will contain the percentage of the traning set we are going to have\n",
    "# to test on other traning splits add the traning set percentage to the array\n",
    "traning_set_splits = [90]\n",
    "\n",
    "# this array contains the preprocessing tags we are going to use\n",
    "# in order the tasgs refer to [clean, stop, lemmatization, stemming, special]\n",
    "preprcessing_configs = [\n",
    "\t    [[False, False, False, False, False]],\n",
    "\t        \n",
    "\t    [[True, False, False, False, False],\n",
    "\t    [False, True, False, False, False],\n",
    "\t    [False, False, True, False, False], \n",
    "\t    [False, False, False, True, False],\n",
    "\t    [False, False, False, False, True]],\n",
    "\t              \n",
    "\t    [[True, False, True, False, True],\n",
    "\t    [True, False, False, True, True],\n",
    "\t    [False, True, False, True, True]]\n",
    "            ]\n",
    "\n",
    "# Run and evaluate\n",
    "results = []\n",
    "\n",
    "for i in traning_set_splits:\n",
    "    for m in preprcessing_configs:\n",
    "        \n",
    "        config = []\n",
    "        percent = []\n",
    "        accuracy = []\n",
    "\n",
    "        for j in m:\n",
    "\n",
    "            # slip data and create testing and traning set\n",
    "            train_reviews, train_sentiments, test_reviews, test_sentiments = split_data(i)\n",
    "\n",
    "            # preprocess datasets\n",
    "            processed_train_reviews = preprocess_corpus(train_reviews, j[0], j[1], j[2], j[3], j[4])\n",
    "            processed_test_reviews = preprocess_corpus(test_reviews, j[0], j[1], j[2], j[3], j[4])\n",
    "            \n",
    "            test_sentiments, predictions = train(processed_train_reviews, processed_test_reviews)\n",
    "            \n",
    "            # print results and data split info\n",
    "            print(\" \")\n",
    "            print(str(i) + \"% Train \" + str(100-i) +\"% Test\" )\n",
    "            print(\"clean=%s, stop=%s, lemmatization=%s, stemming=%s, special=%s\" % (j[0], j[1], j[2], j[3], j[4]))\n",
    "\n",
    "            acc, prec, rec = print_res(predictions, test_sentiments)\n",
    "\n",
    "            # store results for later on\n",
    "            config.append(\"clean=%s, stop=%s, lemmatization=%s, stemming=%s, special=%s\" % (j[0], j[1], j[2], j[3], j[4]))\n",
    "            accuracy.append(round(acc*100,2))\n",
    "            percent.append((i,100-i))\n",
    "        \n",
    "            output = {'Config':config, 'Percent':percent, 'Accuracy':accuracy}\n",
    "            output_df = pd.DataFrame(output)\n",
    "            \n",
    "        results.append(output_df)\n",
    "\n",
    "\n",
    "# print all the results orderd by 'Accuracy'\n",
    "for i in results:\n",
    "    pd.set_option('max_colwidth', 500)\n",
    "\n",
    "    output_df.style.set_properties(subset=['Config'], **{'width': '600px'})\n",
    "\n",
    "    display(i.sort_values('Accuracy'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae12626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
